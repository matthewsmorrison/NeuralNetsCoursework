import matplotlib.pyplot as plt


learningRates = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]
data_dict = get_CIFAR10_data()

losses = []
accuracies = []

for learningRate in learningRates:
    model = FullyConnectedNet([120],reg=0.5, dtype=np.float64)
    number_epochs = 30
    solver = Solver(model,data_dict,optim_config={'learning_rate':learningRate},lr_decay =0.99,num_epochs=number_epochs,batch_size=200,print_every=5000,num_train_samples=40000)
    losses.append(solver.loss_history)
    accuracies.append(solver.val_acc_history)


plt.subplot(2,1,1)
plt.title("Training loss")
for loss in losses:
    plt.plot(loss,'-o',label='train')
plt.subplot(2,1,2)
for accuracy in accuracies:
    plt.plot(accuracy,'-o',label='val')
plt.plot([0.5]* len(solver.val_acc_history),'k--')
plt.xlabel('Epoch')
plt.xlim(0,number_epochs)
#plt.plot(solver.loss_history,'-o',label='train')

plt.legend(loc='lower right')
plt.gcf().set_size_inches(15,12)
plt.show()
